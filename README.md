# **Adversarial Attack and Defense**

This project is an interactive application built with Streamlit to showcase the effects of adversarial attacks on a Convolutional Neural Network (CNN) trained on the CIFAR-100 dataset. Users can upload images or select from predefined samples, apply adversarial attacks such as the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), and observe the modelâ€™s responses with Grad-CAM visualizations. The project aims to educate users about adversarial machine learning, demonstrating how small, nearly invisible changes to images can deceive deep learning models.

## **Key Features**

- **Interactive Image Selection**: Upload your own image or choose from a set of sample images.
- **Adversarial Attack Configuration**: Apply FGSM or PGD attacks with adjustable parameters.
- **Side-by-Side Comparison**: View original and adversarial images along with model predictions.
- **Grad-CAM Visualizations**: Visualize the model's focus areas before and after the attack to understand how adversarial perturbations influence CNN predictions.
- **Educational Content**: Learn about adversarial vulnerabilities, attack mechanisms, and defense strategies.

For detailed documentation, technical explanations, and future enhancement ideas, please refer to the [Notion Documentation](https://tinyurl.com/AdversarialAttack-Defense)

---

